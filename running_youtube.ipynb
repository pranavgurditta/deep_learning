{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "running_youtube.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pranavgurditta/deep_learning/blob/master/running_youtube.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWbYTknvaxYA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "06714d11-d28a-442d-e53f-3e74267e9f3a"
      },
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install --upgrade google-api-python-client"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: google-api-python-client in /usr/local/lib/python3.6/dist-packages (1.7.11)\n",
            "Requirement already satisfied, skipping upgrade: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client) (1.4.2)\n",
            "Requirement already satisfied, skipping upgrade: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client) (3.0.0)\n",
            "Requirement already satisfied, skipping upgrade: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client) (0.11.3)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client) (0.0.3)\n",
            "Requirement already satisfied, skipping upgrade: cachetools>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client) (3.1.1)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client) (0.2.7)\n",
            "Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client) (4.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.4.1->google-api-python-client) (0.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amO8j-xpaxYC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "31acac83-80ae-46a6-88f2-1a7e02e9868a"
      },
      "source": [
        "pip install --upgrade google-api-python-client\n"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: google-api-python-client in /usr/local/lib/python3.6/dist-packages (1.7.11)\n",
            "Requirement already satisfied, skipping upgrade: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client) (3.0.0)\n",
            "Requirement already satisfied, skipping upgrade: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client) (0.11.3)\n",
            "Requirement already satisfied, skipping upgrade: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client) (1.4.2)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client) (0.0.3)\n",
            "Requirement already satisfied, skipping upgrade: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client) (0.2.7)\n",
            "Requirement already satisfied, skipping upgrade: cachetools>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client) (3.1.1)\n",
            "Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client) (4.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.4.1->google-api-python-client) (0.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzTBb7NXaxYE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "52ce96e8-32c5-4b42-8513-2c5e54c2d3a3"
      },
      "source": [
        "pip install google_auth_oauthlib"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: google_auth_oauthlib in /usr/local/lib/python3.6/dist-packages (0.4.1)\n",
            "Requirement already satisfied: google-auth in /usr/local/lib/python3.6/dist-packages (from google_auth_oauthlib) (1.4.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google_auth_oauthlib) (1.2.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth->google_auth_oauthlib) (0.2.7)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from google-auth->google_auth_oauthlib) (1.12.0)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth->google_auth_oauthlib) (4.0)\n",
            "Requirement already satisfied: cachetools>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth->google_auth_oauthlib) (3.1.1)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google_auth_oauthlib) (2.21.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google_auth_oauthlib) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth->google_auth_oauthlib) (0.4.7)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google_auth_oauthlib) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google_auth_oauthlib) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google_auth_oauthlib) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google_auth_oauthlib) (2019.9.11)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkAeSIV6axYG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "46fe4769-9638-4732-9e56-9fc80ec5dc50"
      },
      "source": [
        "pip install oauth2client"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.6/dist-packages (4.1.3)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client) (0.4.7)\n",
            "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client) (1.12.0)\n",
            "Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client) (0.11.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client) (0.2.7)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client) (4.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKYlvkMBaxYI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "9123717d-e63f-4652-be90-94d9d35eface"
      },
      "source": [
        "pip install pandas\n"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.17.3)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.6.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ckq0UDmB57E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "91c573bb-976c-4724-bcdf-6947f06a19c9"
      },
      "source": [
        "pip install vaderSentiment"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting vaderSentiment\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/86/9e/c53e1fc61aac5ee490a6ac5e21b1ac04e55a7c2aba647bb8411c9aadf24e/vaderSentiment-3.2.1-py2.py3-none-any.whl (125kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 11.7MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 30kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 51kB 2.1MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 61kB 2.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 71kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 81kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 92kB 3.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 2.8MB/s \n",
            "\u001b[?25hInstalling collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5kclKnicOsa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "86bddf35-f460-423b-c264-e17e420e2b69"
      },
      "source": [
        "pip install afinn"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: afinn in /usr/local/lib/python3.6/dist-packages (0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4Er9hq7c0-u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "outputId": "cc6f9051-397a-4292-ef1e-51ecc30d9934"
      },
      "source": [
        "pip install lime"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: lime in /usr/local/lib/python3.6/dist-packages (0.1.1.36)\n",
            "Requirement already satisfied: scikit-image>=0.12; python_version >= \"3.0\" in /usr/local/lib/python3.6/dist-packages (from lime) (0.15.0)\n",
            "Requirement already satisfied: matplotlib; python_version >= \"3.0\" in /usr/local/lib/python3.6/dist-packages (from lime) (3.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from lime) (1.17.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from lime) (1.3.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.6/dist-packages (from lime) (0.21.3)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.12; python_version >= \"3.0\"->lime) (4.3.0)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.12; python_version >= \"3.0\"->lime) (2.4)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.12; python_version >= \"3.0\"->lime) (1.1.1)\n",
            "Requirement already satisfied: imageio>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.12; python_version >= \"3.0\"->lime) (2.4.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib; python_version >= \"3.0\"->lime) (2.6.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib; python_version >= \"3.0\"->lime) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib; python_version >= \"3.0\"->lime) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib; python_version >= \"3.0\"->lime) (2.4.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.18->lime) (0.14.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.3.0->scikit-image>=0.12; python_version >= \"3.0\"->lime) (0.46)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image>=0.12; python_version >= \"3.0\"->lime) (4.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib; python_version >= \"3.0\"->lime) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib; python_version >= \"3.0\"->lime) (41.4.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1i5ySCOdJhM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "f52dbe48-723f-4cba-e96c-23f7575fc277"
      },
      "source": [
        "pip install pyLDavis"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyLDavis in /usr/local/lib/python3.6/dist-packages (2.1.2)\n",
            "Requirement already satisfied: funcy in /usr/local/lib/python3.6/dist-packages (from pyLDavis) (1.13)\n",
            "Requirement already satisfied: pandas>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from pyLDavis) (0.25.2)\n",
            "Requirement already satisfied: wheel>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from pyLDavis) (0.33.6)\n",
            "Requirement already satisfied: joblib>=0.8.4 in /usr/local/lib/python3.6/dist-packages (from pyLDavis) (0.14.0)\n",
            "Requirement already satisfied: numpy>=1.9.2 in /usr/local/lib/python3.6/dist-packages (from pyLDavis) (1.17.3)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.6/dist-packages (from pyLDavis) (2.7.0)\n",
            "Requirement already satisfied: scipy>=0.18.0 in /usr/local/lib/python3.6/dist-packages (from pyLDavis) (1.3.1)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from pyLDavis) (3.6.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyLDavis) (0.16.0)\n",
            "Requirement already satisfied: jinja2>=2.7.2 in /usr/local/lib/python3.6/dist-packages (from pyLDavis) (2.10.3)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.17.0->pyLDavis) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.17.0->pyLDavis) (2.6.1)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDavis) (0.7.1)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDavis) (1.8.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDavis) (1.3.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDavis) (7.2.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDavis) (19.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDavis) (41.4.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDavis) (1.12.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.7.2->pyLDavis) (1.1.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZj9_6oXaxYK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "613e5b8a-9a1f-4c59-da67-7992b4ab0342"
      },
      "source": [
        "from apiclient.discovery import build\n",
        "from apiclient.errors import HttpError\n",
        "from oauth2client.tools import argparser\n",
        "import pandas as pd\n",
        "import pprint \n",
        "import matplotlib.pyplot as pd\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import spacy\n",
        "import nltk\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import unicodedata\n",
        "\n",
        "nlp = spacy.load('en', parse = False, tag=False, entity=False)\n",
        "tokenizer = ToktokTokenizer()\n",
        "\n",
        "import datetime\n",
        "from datetime import timedelta\n",
        " \n",
        "datetimeFormat = '%Y-%m-%d %H:%M:%S.%f'\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, auc, accuracy_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.decomposition import NMF\n",
        "from sklearn.base import clone\n",
        "\n",
        "from scipy import interp\n",
        "\n",
        "from afinn import Afinn\n",
        "afn = Afinn(emoticons=True) \n",
        "\n",
        "import nltk\n",
        "nltk.download('all', halt_on_error=False)\n",
        "from nltk.corpus import sentiwordnet as swn\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "import gensim\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "from IPython.display import SVG\n",
        "\n",
        "#from skater.core.local_interpretation.lime.lime_text import LimeTextExplainer\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "\n",
        "import pyLDAvis\n",
        "import pyLDAvis.sklearn\n",
        "\n",
        "np.set_printoptions(precision=2, linewidth=80)\n",
        "\n"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Package abc is already up-to-date!\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Package alpino is already up-to-date!\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Package chat80 is already up-to-date!\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package city_database is already up-to-date!\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    |   Package comtrans is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Package crubadan is already up-to-date!\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Package dolch is already up-to-date!\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Package floresta is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Package ieer is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Package indian is already up-to-date!\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    |   Package jeita is already up-to-date!\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Package kimmo is already up-to-date!\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    |   Package knbc is already up-to-date!\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    |   Package machado is already up-to-date!\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Package paradigms is already up-to-date!\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Package pil is already up-to-date!\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Package pl196x is already up-to-date!\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Package ppattach is already up-to-date!\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    |   Package propbank is already up-to-date!\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Package ptb is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Package qc is already up-to-date!\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    |   Package reuters is already up-to-date!\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Package rte is already up-to-date!\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    |   Package semcor is already up-to-date!\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Package senseval is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Package smultron is already up-to-date!\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Package state_union is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Package swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Package switchboard is already up-to-date!\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Package timit is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Package toolbox is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Package webtext is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Package ycoe is already up-to-date!\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Package rslp is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Package tagsets is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Package porter_test is already up-to-date!\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asfjKfXrfqji",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def strip_html_tags(text):\n",
        "    '''\n",
        "    Our text often contains unnecessary content like HTML tags, which do not\n",
        "    add much value when analyzing sentiment. Hence we need to make sure we remove\n",
        "    them before extracting features. The BeautifulSoup library does an excellent job in\n",
        "    providing necessary functions for this. Our strip_html_tags(...) function enables in cleaning and stripping out HTML code.\n",
        "    '''\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    stripped_text = soup.get_text()\n",
        "    return stripped_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRiPXvNngAjL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_accented_chars(text):\n",
        "    '''\n",
        "    In our dataset, we are dealing with reviews in the English language \n",
        "    so we need to make sure that characters with any other format, especially accented \n",
        "    characters are converted and standardized \n",
        "    into ASCII characters. A simple example would be converting é to e. \n",
        "    Our remove_accented_chars(...) function helps us in this respect.\n",
        "    '''  \n",
        "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Ep5XiZjgjs5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "CONTRACTION_MAP = {\n",
        "\"ain't\": \"is not\",\n",
        "\"aren't\": \"are not\",\n",
        "\"can't\": \"cannot\",\n",
        "\"can't've\": \"cannot have\",\n",
        "\"'cause\": \"because\",\n",
        "\"could've\": \"could have\",\n",
        "\"couldn't\": \"could not\",\n",
        "\"couldn't've\": \"could not have\",\n",
        "\"didn't\": \"did not\",\n",
        "\"doesn't\": \"does not\",\n",
        "\"don't\": \"do not\",\n",
        "\"hadn't\": \"had not\",\n",
        "\"hadn't've\": \"had not have\",\n",
        "\"hasn't\": \"has not\",\n",
        "\"haven't\": \"have not\",\n",
        "\"he'd\": \"he would\",\n",
        "\"he'd've\": \"he would have\",\n",
        "\"he'll\": \"he will\",\n",
        "\"he'll've\": \"he will have\",\n",
        "\"he's\": \"he is\",\n",
        "\"how'd\": \"how did\",\n",
        "\"how'd'y\": \"how do you\",\n",
        "\"how'll\": \"how will\",\n",
        "\"how's\": \"how is\",\n",
        "\"I'd\": \"I would\",\n",
        "\"I'd've\": \"I would have\",\n",
        "\"I'll\": \"I will\",\n",
        "\"I'll've\": \"I will have\",\n",
        "\"I'm\": \"I am\",\n",
        "\"I've\": \"I have\",\n",
        "\"i'd\": \"i would\",\n",
        "\"i'd've\": \"i would have\",\n",
        "\"i'll\": \"i will\",\n",
        "\"i'll've\": \"i will have\",\n",
        "\"i'm\": \"i am\",\n",
        "\"i've\": \"i have\",\n",
        "\"isn't\": \"is not\",\n",
        "\"it'd\": \"it would\",\n",
        "\"it'd've\": \"it would have\",\n",
        "\"it'll\": \"it will\",\n",
        "\"it'll've\": \"it will have\",\n",
        "\"it's\": \"it is\",\n",
        "\"let's\": \"let us\",\n",
        "\"ma'am\": \"madam\",\n",
        "\"mayn't\": \"may not\",\n",
        "\"might've\": \"might have\",\n",
        "\"mightn't\": \"might not\",\n",
        "\"mightn't've\": \"might not have\",\n",
        "\"must've\": \"must have\",\n",
        "\"mustn't\": \"must not\",\n",
        "\"mustn't've\": \"must not have\",\n",
        "\"needn't\": \"need not\",\n",
        "\"needn't've\": \"need not have\",\n",
        "\"o'clock\": \"of the clock\",\n",
        "\"oughtn't\": \"ought not\",\n",
        "\"oughtn't've\": \"ought not have\",\n",
        "\"shan't\": \"shall not\",\n",
        "\"sha'n't\": \"shall not\",\n",
        "\"shan't've\": \"shall not have\",\n",
        "\"she'd\": \"she would\",\n",
        "\"she'd've\": \"she would have\",\n",
        "\"she'll\": \"she will\",\n",
        "\"she'll've\": \"she will have\",\n",
        "\"she's\": \"she is\",\n",
        "\"should've\": \"should have\",\n",
        "\"shouldn't\": \"should not\",\n",
        "\"shouldn't've\": \"should not have\",\n",
        "\"so've\": \"so have\",\n",
        "\"so's\": \"so as\",\n",
        "\"that'd\": \"that would\",\n",
        "\"that'd've\": \"that would have\",\n",
        "\"that's\": \"that is\",\n",
        "\"there'd\": \"there would\",\n",
        "\"there'd've\": \"there would have\",\n",
        "\"there's\": \"there is\",\n",
        "\"they'd\": \"they would\",\n",
        "\"they'd've\": \"they would have\",\n",
        "\"they'll\": \"they will\",\n",
        "\"they'll've\": \"they will have\",\n",
        "\"they're\": \"they are\",\n",
        "\"they've\": \"they have\",\n",
        "\"to've\": \"to have\",\n",
        "\"wasn't\": \"was not\",\n",
        "\"we'd\": \"we would\",\n",
        "\"we'd've\": \"we would have\",\n",
        "\"we'll\": \"we will\",\n",
        "\"we'll've\": \"we will have\",\n",
        "\"we're\": \"we are\",\n",
        "\"we've\": \"we have\",\n",
        "\"weren't\": \"were not\",\n",
        "\"what'll\": \"what will\",\n",
        "\"what'll've\": \"what will have\",\n",
        "\"what're\": \"what are\",\n",
        "\"what's\": \"what is\",\n",
        "\"what've\": \"what have\",\n",
        "\"when's\": \"when is\",\n",
        "\"when've\": \"when have\",\n",
        "\"where'd\": \"where did\",\n",
        "\"where's\": \"where is\",\n",
        "\"where've\": \"where have\",\n",
        "\"who'll\": \"who will\",\n",
        "\"who'll've\": \"who will have\",\n",
        "\"who's\": \"who is\",\n",
        "\"who've\": \"who have\",\n",
        "\"why's\": \"why is\",\n",
        "\"why've\": \"why have\",\n",
        "\"will've\": \"will have\",\n",
        "\"won't\": \"will not\",\n",
        "\"won't've\": \"will not have\",\n",
        "\"would've\": \"would have\",\n",
        "\"wouldn't\": \"would not\",\n",
        "\"wouldn't've\": \"would not have\",\n",
        "\"y'all\": \"you all\",\n",
        "\"y'all'd\": \"you all would\",\n",
        "\"y'all'd've\": \"you all would have\",\n",
        "\"y'all're\": \"you all are\",\n",
        "\"y'all've\": \"you all have\",\n",
        "\"you'd\": \"you would\",\n",
        "\"you'd've\": \"you would have\",\n",
        "\"you'll\": \"you will\",\n",
        "\"you'll've\": \"you will have\",\n",
        "\"you're\": \"you are\",\n",
        "\"you've\": \"you have\"\n",
        "}\n",
        "\n",
        "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
        "    '''\n",
        "    Expanding Contractions¶\n",
        "    In the English language, contractions are basically shortened versions of words or syllables. \n",
        "    Contractions pose a problem in text normalization because we have to deal with special characters like the apostrophe\n",
        "    and we also have to convert each contraction to its expanded, original form. \n",
        "    Our expand_contractions(...) function uses regular expressions and various contractions mapped to expand all contractions in our text corpus.\n",
        "    '''\n",
        "    \n",
        "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
        "                           flags=re.IGNORECASE|re.DOTALL) #re.compile takes and argument and flags and converts the argument to a pattern object,for example\n",
        "                           # the pattern object has pattern (are'nt | haven't |...) and as flag has IGNORECASE enable [A-Z] can map to small a to z too and \n",
        "                           # DOTALL allows '.' to refer every character and new line too.\n",
        "\n",
        "\n",
        "    def expand_match(contraction):\n",
        "        match = contraction.group(0)\n",
        "        first_char = match[0]\n",
        "        expanded_contraction = contraction_mapping.get(match)\\\n",
        "                                if contraction_mapping.get(match)\\\n",
        "                                else contraction_mapping.get(match.lower())                       \n",
        "        expanded_contraction = first_char+expanded_contraction[1:]\n",
        "        return expanded_contraction\n",
        "        \n",
        "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
        "    expanded_text = re.sub(\"'\",\"\", expanded_text)\n",
        "    return expanded_text\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILQzvV85zq1e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_special_characters(text):\n",
        "    '''\n",
        "    Removing Special Characters\n",
        "    Simple regexes can be used to achieve this. Our function remove_special_characters(...) helps us remove special characters.\n",
        "    In our code, we have retained numbers but you can also remove numbers if you do not want them in your normalized corpus.\n",
        "    '''\n",
        "    text = re.sub(r'[^a-zA-z0-9\\s]', '', text)\n",
        "    return text\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeWfEnZJ0bNj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lemmatize_text(text):\n",
        "    '''\n",
        "    Lemmatizing text\n",
        "    Word stems are usually the base form of possible words that can be created by attaching affixes like prefixes and\n",
        "    suffixes to the stem to create new words. This is known as inflection. The reverse process of obtaining the base form of a\n",
        "    word is known as stemming. The nltk package offers a wide range of stemmers like the PorterStemmer and LancasterStemmer. \n",
        "    Lemmatization is very similar to stemming, where we remove word affixes to get to the base form of a word. However the base\n",
        "    form in this case is known as the root word but not the root stem. The difference being that the root word is always a\n",
        "    lexicographically correct word, present in the dictionary, but the root stem may not be so. We will be using lemmatization only \n",
        "    in our normalization pipeline to retain lexicographically correct words. The function lemmatize_text(...) helps us with this aspect\n",
        "    '''\n",
        "    text = nlp(text)\n",
        "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WY7OfXeh0p9O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stopword_list = nltk.corpus.stopwords.words('english')\n",
        "stopword_list.remove('no')\n",
        "stopword_list.remove('not')\n",
        "\n",
        "def remove_stopwords(text, is_lower_case=False):\n",
        "    '''\n",
        "\n",
        "    Words which have little or no significance especially when constructing meaningful features from text \n",
        "    are also known as stopwords or stop words. These are usually words that end up having the maximum frequency if you do\n",
        "     a simple term or word frequency in a document corpus. Words like a, an, the, and so on are considered to be stopwords. \n",
        "     There is no universal stopword list but we use a standard English language stopwords list from nltk. You can also add\n",
        "      your own domain specific stopwords if needed. \n",
        "    The function remove_stopwords(...) helps us remove stopwords and retain words having the most significance and context in a corpus.\n",
        "    '''\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tokens = [token.strip() for token in tokens]\n",
        "    if is_lower_case:\n",
        "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
        "    else:\n",
        "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
        "    filtered_text = ' '.join(filtered_tokens)    \n",
        "    return filtered_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXmklQVD06wV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize_corpus(corpus, html_stripping=True, contraction_expansion=True,\n",
        "                     accented_char_removal=True, text_lower_case=True, \n",
        "                     text_lemmatization=True, special_char_removal=True, \n",
        "                     stopword_removal=True):\n",
        "    \n",
        "    normalized_corpus = []\n",
        "    # normalize each document in the corpus\n",
        "    doc=corpus\n",
        "        # strip HTML\n",
        "    if html_stripping:\n",
        "        doc = strip_html_tags(doc)\n",
        "            \n",
        "        # remove accented characters\n",
        "    if accented_char_removal:\n",
        "        doc = remove_accented_chars(doc)\n",
        "        # expand contractions    \n",
        "    if contraction_expansion:\n",
        "        doc = expand_contractions(doc)\n",
        "        # lowercase the text    \n",
        "    if text_lower_case:\n",
        "        doc = doc.lower()\n",
        "        # remove extra newlines\n",
        "    doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc)\n",
        "        # insert spaces between special characters to isolate them    \n",
        "    special_char_pattern = re.compile(r'([{.(-)!}])')\n",
        "    doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
        "        # lemmatize text\n",
        "    if text_lemmatization:\n",
        "        doc = lemmatize_text(doc)\n",
        "        # remove special characters    \n",
        "    if special_char_removal:\n",
        "        doc = remove_special_characters(doc)  \n",
        "        # remove extra whitespace\n",
        "    doc = re.sub(' +', ' ', doc)\n",
        "        # remove stopwords\n",
        "    if stopword_removal:\n",
        "        doc = remove_stopwords(doc, is_lower_case=text_lower_case)\n",
        "            \n",
        "    normalized_corpus.append(doc)\n",
        "        \n",
        "    return doc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAnjfGhQCDjO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "def score_reviews(reviews):\n",
        "    score_list = []\n",
        "    overall_score = 0\n",
        "    analyser = SentimentIntensityAnalyzer()\n",
        "    for review in reviews:\n",
        "        score = analyser.polarity_scores(review)\n",
        "        #print(str(score))\n",
        "        score_list.append(score['compound'])\n",
        "    #print(score_list)\n",
        "    for compound_score in score_list:\n",
        "        overall_score = overall_score + compound_score\n",
        "    return (overall_score/len(score_list))\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWlplK_Cem1m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9edca8e0-719d-46e0-c415-8e4cd5ab7d9f"
      },
      "source": [
        "\n",
        "DEVELOPER_KEY = \"AIzaSyDoyNLORaFsQfeNDoOwA78726OO3d6TS3o\"\n",
        "YOUTUBE_API_SERVICE_NAME = \"youtube\"\n",
        "YOUTUBE_API_VERSION = \"v3\"\n",
        "youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION,developerKey=DEVELOPER_KEY)\n",
        "Add here the function which I am commenting below and pass the search result response object or use it where you are appending links"
        "def main():\n",
        "    keyword_user=input(\"Enter the keyword\")\n",
        "    \n",
        "    request = youtube.search().list(part=\"id,snippet\",maxResults=10,q=keyword_user)\n",
        "    video_links=[]\n",
        "    response = request.execute()\n",
        "    print(response['items'])\n",
        "    for item in response['items']:\n",
        "        print(\"the video links are\")\n",
        "        if('videoId' in item['id']):\n",
        "          video_links.append(item['id']['videoId'])  \n",
        "          print(item['id']['videoId'])\n",
        "          c=0\n",
        "    video_rating_comment_based=[]\n",
        "    for video in video_links:\n",
        "        try:\n",
        "          request = youtube.commentThreads().list(part=\"snippet,replies\",videoId=video)\n",
        "          comments_of_single_video = request.execute()\n",
        "        except:\n",
        "          video_rating_comment_based.append(-999999)\n",
        "          continue\n",
        "\n",
        "        print(\"The comments are\")   \n",
        "        comments_to_evaluate=[]\n",
        "        for single_comment in comments_of_single_video['items']:\n",
        "            a = normalize_corpus(single_comment['snippet']['topLevelComment']['snippet']['textDisplay'])\n",
        "            print((a))\n",
        "            comments_to_evaluate.append(a)\n",
        "\n",
        "        print(score_reviews(comments_to_evaluate))\n",
        "        video_rating_comment_based.append(score_reviews(comments_to_evaluate))\n",
        "\n",
        "    url_rating_dict={}\n",
        "    for url in video_links:\n",
        "      for rating in video_rating_comment_based:\n",
        "        url_rating_dict[url]=rating\n",
        "        video_rating_comment_based.remove(rating) \n",
        "        break  \n",
        "    return (url_rating_dict)\n",
        "\n",
        "def remNonChar(inputString):\n",
        "    return inputString.encode('ascii', 'ignore').decode('ascii')\n",
        "    \n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "    "
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter the keywordspiderman 3\n",
            "[{'kind': 'youtube#searchResult', 'etag': '\"j6xRRd8dTPVVptg711_CSPADRfg/8wBVJxBz_Ypp0t7uDVQgnUBsMzs\"', 'id': {'kind': 'youtube#playlist', 'playlistId': 'PL3euYSnILSRRS0gq_p-VqNjbbHl-QISGn'}, 'snippet': {'publishedAt': '2018-10-21T18:05:13.000Z', 'channelId': 'UCRFMkWoYjcme_24BTTh_h5Q', 'title': 'Spider-Man 3 (2007) FULL MOVIE PLAYLIST', 'description': '', 'thumbnails': {'default': {'url': 'https://i.ytimg.com/vi/GQyl0R-H1uY/default.jpg', 'width': 120, 'height': 90}, 'medium': {'url': 'https://i.ytimg.com/vi/GQyl0R-H1uY/mqdefault.jpg', 'width': 320, 'height': 180}, 'high': {'url': 'https://i.ytimg.com/vi/GQyl0R-H1uY/hqdefault.jpg', 'width': 480, 'height': 360}}, 'channelTitle': 'Matthew Salinas', 'liveBroadcastContent': 'none'}}, {'kind': 'youtube#searchResult', 'etag': '\"j6xRRd8dTPVVptg711_CSPADRfg/Uc8vDXLeqTNRsMa8maVarbjFQVo\"', 'id': {'kind': 'youtube#video', 'videoId': 'bkRRMOcmgoc'}, 'snippet': {'publishedAt': '2019-10-01T15:49:03.000Z', 'channelId': 'UCDmd40nGOGL3IxKNVeRb9-w', 'title': 'Sam Raimi&#39;s Spider-Man 3 - The Almost Perfect Finale (Part 3)', 'description': 'Sam Raimi went through a lot when making Spider-Man 3. This is the finale, the almost perfect finale to the HiTop Films Spider-Man Series.The last HiTop Films ...', 'thumbnails': {'default': {'url': 'https://i.ytimg.com/vi/bkRRMOcmgoc/default.jpg', 'width': 120, 'height': 90}, 'medium': {'url': 'https://i.ytimg.com/vi/bkRRMOcmgoc/mqdefault.jpg', 'width': 320, 'height': 180}, 'high': {'url': 'https://i.ytimg.com/vi/bkRRMOcmgoc/hqdefault.jpg', 'width': 480, 'height': 360}}, 'channelTitle': 'HiTop Films', 'liveBroadcastContent': 'none'}}, {'kind': 'youtube#searchResult', 'etag': '\"j6xRRd8dTPVVptg711_CSPADRfg/CNO37GKIC_3hrSVmP-9L1E3hDgQ\"', 'id': {'kind': 'youtube#video', 'videoId': 'YmeInnwnIS8'}, 'snippet': {'publishedAt': '2019-10-03T19:45:01.000Z', 'channelId': 'UCuCk_7b2_4uSr6y5hFmjuMQ', 'title': 'What To Expect In Marvel&#39;s Spider-Man 3', 'description': \"Marvel's Spider-Man 3 With Tom Holland In The MCU: Everything You Need To Know SUBSCRIBE NOW to CBR! Click here: http://bit.ly/Subscribe-to-CBR We're ...\", 'thumbnails': {'default': {'url': 'https://i.ytimg.com/vi/YmeInnwnIS8/default.jpg', 'width': 120, 'height': 90}, 'medium': {'url': 'https://i.ytimg.com/vi/YmeInnwnIS8/mqdefault.jpg', 'width': 320, 'height': 180}, 'high': {'url': 'https://i.ytimg.com/vi/YmeInnwnIS8/hqdefault.jpg', 'width': 480, 'height': 360}}, 'channelTitle': 'CBR', 'liveBroadcastContent': 'none'}}, {'kind': 'youtube#searchResult', 'etag': '\"j6xRRd8dTPVVptg711_CSPADRfg/ykq5oJJpeIdA-5Ma_r96_jVGc24\"', 'id': {'kind': 'youtube#video', 'videoId': '8HNmSR0ZDV8'}, 'snippet': {'publishedAt': '2019-10-25T14:00:01.000Z', 'channelId': 'UCP1iRaFlS5EYjJBryFV9JPw', 'title': 'Spider-Man 3 Details Revealed', 'description': \"Sony and Disney found their way back to the bargaining table, and a deal has been struck. Ol' web-head will get at least one more MCU-set starring vehicle, one ...\", 'thumbnails': {'default': {'url': 'https://i.ytimg.com/vi/8HNmSR0ZDV8/default.jpg', 'width': 120, 'height': 90}, 'medium': {'url': 'https://i.ytimg.com/vi/8HNmSR0ZDV8/mqdefault.jpg', 'width': 320, 'height': 180}, 'high': {'url': 'https://i.ytimg.com/vi/8HNmSR0ZDV8/hqdefault.jpg', 'width': 480, 'height': 360}}, 'channelTitle': 'Looper', 'liveBroadcastContent': 'none'}}, {'kind': 'youtube#searchResult', 'etag': '\"j6xRRd8dTPVVptg711_CSPADRfg/0Le1v36RuL5c45yXcqOQyMFnweQ\"', 'id': {'kind': 'youtube#video', 'videoId': 'GVt3XFTYp-0'}, 'snippet': {'publishedAt': '2016-12-08T22:54:32.000Z', 'channelId': 'UC3gNmTGu-TTbFPpfSs5kNkg', 'title': 'Spider-Man 3 (2007) - The End of Spider-Man? Scene (8/10) | Movieclips', 'description': \"Spider-Man 3 movie clips: http://j.mp/2gcj2nT BUY THE MOVIE: http://bit.ly/2hbLpzF Don't miss the HOTTEST NEW TRAILERS: http://bit.ly/1u2y6pr CLIP ...\", 'thumbnails': {'default': {'url': 'https://i.ytimg.com/vi/GVt3XFTYp-0/default.jpg', 'width': 120, 'height': 90}, 'medium': {'url': 'https://i.ytimg.com/vi/GVt3XFTYp-0/mqdefault.jpg', 'width': 320, 'height': 180}, 'high': {'url': 'https://i.ytimg.com/vi/GVt3XFTYp-0/hqdefault.jpg', 'width': 480, 'height': 360}}, 'channelTitle': 'Movieclips', 'liveBroadcastContent': 'none'}}, {'kind': 'youtube#searchResult', 'etag': '\"j6xRRd8dTPVVptg711_CSPADRfg/CdoToDNerkNkjpqLC7HlExrFjtQ\"', 'id': {'kind': 'youtube#video', 'videoId': 'oqdiN5cFgZs'}, 'snippet': {'publishedAt': '2017-07-29T16:49:36.000Z', 'channelId': 'UClVbhSLxwws-KSsPKz135bw', 'title': 'Eddie Brock Becomes Venom (Scene) - Spider-Man 3 (2007) Movie CLIP HD', 'description': 'Eddie Brock Becomes Venom - Venom Rises - Spider-Man 3 (2007) Movie CLIP [1080p HD ] TM & © Sony (2007) Fair use. Copyright Disclaimer Under Section ...', 'thumbnails': {'default': {'url': 'https://i.ytimg.com/vi/oqdiN5cFgZs/default.jpg', 'width': 120, 'height': 90}, 'medium': {'url': 'https://i.ytimg.com/vi/oqdiN5cFgZs/mqdefault.jpg', 'width': 320, 'height': 180}, 'high': {'url': 'https://i.ytimg.com/vi/oqdiN5cFgZs/hqdefault.jpg', 'width': 480, 'height': 360}}, 'channelTitle': 'TopMovieClips', 'liveBroadcastContent': 'none'}}, {'kind': 'youtube#searchResult', 'etag': '\"j6xRRd8dTPVVptg711_CSPADRfg/NYPOOr9Mrvw9i-igWk15TFnBeGU\"', 'id': {'kind': 'youtube#video', 'videoId': 'zkeiEBaOOXA'}, 'snippet': {'publishedAt': '2017-07-10T15:21:02.000Z', 'channelId': 'UClVbhSLxwws-KSsPKz135bw', 'title': 'Spider-Man Gets His Black Suit Scene - Spider-Man 3 (2007) Movie CLIP HD', 'description': 'Spider-Man 3 (2007) - The Black Suit Scene - Spider-Man Gets His Black Suit - Movie CLIP HD [1080p HD ] TM & © Sony (2007) I Do Not Own Anything.All the ...', 'thumbnails': {'default': {'url': 'https://i.ytimg.com/vi/zkeiEBaOOXA/default.jpg', 'width': 120, 'height': 90}, 'medium': {'url': 'https://i.ytimg.com/vi/zkeiEBaOOXA/mqdefault.jpg', 'width': 320, 'height': 180}, 'high': {'url': 'https://i.ytimg.com/vi/zkeiEBaOOXA/hqdefault.jpg', 'width': 480, 'height': 360}}, 'channelTitle': 'TopMovieClips', 'liveBroadcastContent': 'none'}}, {'kind': 'youtube#searchResult', 'etag': '\"j6xRRd8dTPVVptg711_CSPADRfg/lhIk6Y7oltn2n9iDr7r32bWXrr8\"', 'id': {'kind': 'youtube#video', 'videoId': 'jo4134QHwYU'}, 'snippet': {'publishedAt': '2019-02-09T13:00:04.000Z', 'channelId': 'UCiCSDcAcGDvD_v0TQQ8nxJg', 'title': 'Spider-Man vs Sandman - First Fight Scene | SPIDER-MAN 3 (2007) Movie CLIP HD', 'description': 'Spider-Man vs Sandman - First Fight Scene | SPIDER-MAN 3 (2007) Movie CLIP [4K ULTRA HD] Movie info: https://www.imdb.com/title/tt0413300/ Buy it on ...', 'thumbnails': {'default': {'url': 'https://i.ytimg.com/vi/jo4134QHwYU/default.jpg', 'width': 120, 'height': 90}, 'medium': {'url': 'https://i.ytimg.com/vi/jo4134QHwYU/mqdefault.jpg', 'width': 320, 'height': 180}, 'high': {'url': 'https://i.ytimg.com/vi/jo4134QHwYU/hqdefault.jpg', 'width': 480, 'height': 360}}, 'channelTitle': 'BestClips', 'liveBroadcastContent': 'none'}}, {'kind': 'youtube#searchResult', 'etag': '\"j6xRRd8dTPVVptg711_CSPADRfg/i-9Ix7vvF_V884ZE4t_xTLx8c60\"', 'id': {'kind': 'youtube#video', 'videoId': 'm8LWjDS3IkQ'}, 'snippet': {'publishedAt': '2016-12-08T23:03:40.000Z', 'channelId': 'UC3gNmTGu-TTbFPpfSs5kNkg', 'title': 'Spider-Man 3 (2007) - Venom&#39;s Demise Scene (10/10) | Movieclips', 'description': \"Spider-Man 3 movie clips: http://j.mp/2gcj2nT BUY THE MOVIE: http://bit.ly/2hbLpzF Don't miss the HOTTEST NEW TRAILERS: http://bit.ly/1u2y6pr CLIP ...\", 'thumbnails': {'default': {'url': 'https://i.ytimg.com/vi/m8LWjDS3IkQ/default.jpg', 'width': 120, 'height': 90}, 'medium': {'url': 'https://i.ytimg.com/vi/m8LWjDS3IkQ/mqdefault.jpg', 'width': 320, 'height': 180}, 'high': {'url': 'https://i.ytimg.com/vi/m8LWjDS3IkQ/hqdefault.jpg', 'width': 480, 'height': 360}}, 'channelTitle': 'Movieclips', 'liveBroadcastContent': 'none'}}, {'kind': 'youtube#searchResult', 'etag': '\"j6xRRd8dTPVVptg711_CSPADRfg/lfPZt8T7Z_XjlO1YZC92HoAxvDo\"', 'id': {'kind': 'youtube#video', 'videoId': 'mPn5WDCyr2o'}, 'snippet': {'publishedAt': '2017-07-21T16:09:54.000Z', 'channelId': 'UClVbhSLxwws-KSsPKz135bw', 'title': 'Peter Parker vs Harry Osborn - House Fight Scene - Spider-Man 3 (2007) Movie CLIP HD', 'description': 'Peter Parker (Spiderman) vs Harry Osborn (New Goblin) - House Fight Scene - Spider-Man 3 (2007) Movie CLIP HD [1080p HD ] TM & © Sony (2007) Fair use.', 'thumbnails': {'default': {'url': 'https://i.ytimg.com/vi/mPn5WDCyr2o/default.jpg', 'width': 120, 'height': 90}, 'medium': {'url': 'https://i.ytimg.com/vi/mPn5WDCyr2o/mqdefault.jpg', 'width': 320, 'height': 180}, 'high': {'url': 'https://i.ytimg.com/vi/mPn5WDCyr2o/hqdefault.jpg', 'width': 480, 'height': 360}}, 'channelTitle': 'TopMovieClips', 'liveBroadcastContent': 'none'}}]\n",
            "the video links are\n",
            "the video links are\n",
            "bkRRMOcmgoc\n",
            "the video links are\n",
            "YmeInnwnIS8\n",
            "the video links are\n",
            "8HNmSR0ZDV8\n",
            "the video links are\n",
            "GVt3XFTYp-0\n",
            "the video links are\n",
            "oqdiN5cFgZs\n",
            "the video links are\n",
            "zkeiEBaOOXA\n",
            "the video links are\n",
            "jo4134QHwYU\n",
            "the video links are\n",
            "m8LWjDS3IkQ\n",
            "the video links are\n",
            "mPn5WDCyr2o\n",
            "The comments are\n",
            "final sam raimi spider man video thank patience kind word support hope enjoy hope sam somehow see video someday much love alex\n",
            "scene may learn spidey kill flint always kill cus know know pete spidey since yard sale scene 2 fact realise son give hatred ugh kill\n",
            "remade movie good movie present day cgi original cast return make much ad possible\n",
            "3240it always hit hard watch aunt may say start hard thing assume go say apologize instead say forgive damn good\n",
            "fuck idiot film bad\n",
            "like movie low standard\n",
            "hear nostalgia\n",
            "thing agree alot opinion something make really dislike not know\n",
            "hitop film aware fact spider man 4 fanfilm madethe costume look like rip right film\n",
            "dint\n",
            "first spider man movie first movie remember see theater\n",
            "sam raimis superman film\n",
            "643 get around video get n transition amazing\n",
            "give new look use fan vid end though show take time emotional part entire video awesome bro purely genuine\n",
            "hey man want say not let hater strike free thought feeling dislike ffh like sm3 people may disagree time time life not choose live not\n",
            "people always blame sony avi venom make sandman ben killer want test peter mjs relationship despite us sit two previous movie beloved sam raimi spider man 3 never go work long venom include venom make bad\n",
            "100 nice edit look like rlly mvie\n",
            "video reignite love spider man 3\n",
            "3739 excellent writing\n",
            "3719 profoundly beautiful totally agree movie revile think maybe female viewer would like glad see wrong overcome demon truly connect forgiveness fail grow great theme\n",
            "0.21802000000000002\n",
            "The comments are\n",
            "not craven die\n",
            "sinister six bad guy mj could spid woman ne could tech guy van\n",
            "copy film theory\n",
            "taskmaster mean never screen accept ps4\n",
            "mysterio dead make five\n",
            "theme song spid man 1900 spid man spider man friendly neighbor hood spider man spin web every time look come spider maaaaan\n",
            "imagine maximum carnage movie\n",
            "think mysterio not dead plan\n",
            "hm\n",
            "thank upload video\n",
            "maybe sinister six\n",
            "spid man 3 cane alreadylike get joke kf ur spid nerd u know hint old movie spidey diff actor\n",
            "meann would kinda weird someone pop nowhere last movie tiny theory flash could villian\n",
            "spider man leave home\n",
            "hit strange new york\n",
            "peter go tv like not know think spider man\n",
            "question spiderman mcu go become iron man\n",
            "event lead ultimate alliance lead spiderman\n",
            "would like see taskmaster\n",
            "spider man 3 movie amazing spider man 2 movie mcu spiderman 3 movie 2 already show hard find permanent spiderman\n",
            "0.020545\n",
            "The comments are\n",
            "spider man 3 good phase 4 film\n",
            "jeux\n",
            "push back later date not push later date\n",
            "craven chameleon daredevil spider man 3 back home total 6 spidey mcu film 3 venom verse film venom rumor spider man 3 dark film\n",
            "anyone know song use video\n",
            "stop show spiderman god damn spiderman avenger not original spiderman\n",
            "impostor\n",
            "spider man sweet home alabama\n",
            "no mention spider man 3 detail info release month ago\n",
            "bruh many damn spiderman movie go start get repetitive boring\n",
            "press conference put together happy hogan peter hi peter parker spider man take mask ben reilly spider man stuff use b r f tech next scene ben reilly talk nick fury peter well think convince audience ben revert skrull form fury tell peter retire spidey shortly peter get suck sony venom verse\n",
            "spider man sinister six\n",
            "spiderman deadpool movie please\n",
            "spiderman sweet home alabama\n",
            "like u se spiderman cross cinematic universe 10 second later u say sony marvel universe connect mcu man one anotheri hope probably first one cause1 want something not connect mcu sony buy course 2 probably go case cause amy pascal se would like universe vision thank kevin feigy spiderman\n",
            "spider man home invasion\n",
            "someone not like mcu spider man glad get third movie mcu less complicating not spider man fan win tom fan get spider man get 3rd movie feature one mcu movie back sony not ride iron man good side us non tom fan either way spider man fan win no matter\n",
            "suicide squad also come 2021 year go absolutely insane\n",
            "love spiderman yes\n",
            "one mcu movie\n",
            "0.03124999999999999\n",
            "The comments are\n",
            "podria ser el fin del hombre arana\n",
            "much love tom holland spider man still misss tobey\n",
            "venom dieth\n",
            "still good spiderman 2019 make blue\n",
            "p\n",
            "\n",
            "toby good spiderman\n",
            "228 sandman first time rip spider man eye lense\n",
            "spid man\n",
            "remember sandman big fear little\n",
            "dude iconic raimi direct amazing spider man film mcu one sure would better\n",
            "jesus crist good spiderman music freak good\n",
            "holy 256 wak eye\n",
            "\n",
            "not end spiderman man v\n",
            "woah\n",
            "huh never know venom allergic sand\n",
            "venom healing factor\n",
            "158 spidermanhome mary janevenom school\n",
            "not want gfs\n",
            "0.12353499999999999\n",
            "The comments are\n",
            "eddie must really hawk like vision tell peter distance\n",
            "025 legendary part\n",
            "peter swing home necked\n",
            "idoit acter not like guy\n",
            "2014 vs 2019\n",
            "find church bell sound satisfying\n",
            "eddie become venom movie say something like continue time venom spiderman 4 opinion venom not get much time movie\n",
            "miss movie\n",
            "oh god catholic\n",
            "f\n",
            "028 know\n",
            "not know eddie would stand black stuff fall sky\n",
            "symbiote likeoh shiiiii fresh meat\n",
            "mentally dumb pray jesus ask kill someone\n",
            "peter\n",
            "red strict eric screw\n",
            "like version\n",
            "really look forward sony spiderman film not\n",
            "spid man 3 not venom spid man black suit amazing hate spider man 3\n",
            "ending post credit scene film could start cinematic universe\n",
            "0.07474499999999998\n",
            "The comments are\n",
            "clip music well mcu spidey film argue mother\n",
            "214 use durex first time use cheap one year\n",
            "use watch movie time like 4 like least 1 week remember sad ask dad could watch 4th one tell watch even though not fully remember watch entirety movie 4 remember joy would get watch\n",
            "way goo symbiote move incredible kinda satisfy tbh\n",
            "black suit well original suit say n word\n",
            "enter emo phase\n",
            "131 say nigga infront ur black friend not get mad\n",
            "kayeietugriry\n",
            "117 nice back round music\n",
            "hindi move dalo\n",
            "black suit still way cool\n",
            "hear boss music 117\n",
            "scene may look coolbut beginning emo peter meme\n",
            "imagine longer wear less like original suit become like spectacular spider man symbol slowly become venom suit lose webbing keep brick pattern\n",
            "nightmonkey\n",
            "bgm\n",
            "147 look like silver surfer\n",
            "carnage get screen time favorite\n",
            "go black never go back\n",
            "vemom get create\n",
            "0.224425\n",
            "The comments are\n",
            "spider man\n",
            "016 jurassic world\n",
            "love tobey maguire spiderman\n",
            "scene make fight sand kid\n",
            "spiderman vs gaara naruto\n",
            "song call sandman\n",
            "thumbnail ha sandman get fiste no homo heh heh aw sorry guy\n",
            "spiderman\n",
            "sand man action\n",
            "113 talk friend find talk crush\n",
            "sandman technically killer cuz drown cop sand\n",
            "love\n",
            "know harry expose peter say secret identity\n",
            "damn spiderman get thick leg\n",
            "sandman unique concept look not fearful something strange\n",
            "waw abone\n",
            "2007 cgi well 2019 cgi like opinion\n",
            "cleaver shoot collage chamber street\n",
            "sandman get kakyoine thumbnail\n",
            "asu\n",
            "0.013755000000000009\n",
            "The comments are\n",
            "stupid kill bad thing like vennom\n",
            "242he new go blow jump like swimming pool hot summer day\n",
            "way kill venom use extreme loud noise cause symbiote writhe pain scene peter assemble perimeter metal pipe create sonic attack weaken venom use extreme fire peter use pumpkin grenade harry glider throw symbiote cause fire kill mission accomplish\n",
            "218\n",
            "7 year old harry death hit hard\n",
            "time peter happy see razor bat\n",
            "need spidey film venom well villain carnage green goblin doc ock sandman lizard electro vulture already adapt want see team defeat carnage\n",
            "spid man good job kill trashy venom\n",
            "harrys death still hurt\n",
            "eddy special edd guy ever meet\n",
            "anyone else think grow venom eye look like orca\n",
            "venom endi no sola telearana que mejem wason se violado juzgado\n",
            "anyone else feel sorry peter almost lose girlfriend lose good mate\n",
            "dr octopus strong willpower eddie evil not control\n",
            "046 harry need stop hit head\n",
            "eddie not naked without venom\n",
            "question blade not even dtabbe harry notice no one ever see point\n",
            "haaaaaaaaaaaaaarrrrrrrrrrrrryyyyyy\n",
            "carnage come turn movie\n",
            "venom 2\n",
            "-0.21413500000000002\n",
            "The comments are\n",
            "good movie\n",
            "no matter people say enjoy tobey mcguire spidey\n",
            "thanos inevitablepeter 138\n",
            "love movie timing soon harry drop olive drink clink notice peter quietly show behind room favorite spider man movie\n",
            "138 like sister threaten\n",
            "two friend fight girl sound like high school time back\n",
            "look little goblin junior go cry crack every time\n",
            "ooooohh 138\n",
            "look little goblin jr\n",
            "know peter give no fs decide not tell harry truth norman osborn death\n",
            "evil peter parker final villain last spiderman movie\n",
            "strawberry\n",
            "wat not make sense peter throw bomb harry bomb not explode\n",
            "harry throw grenadepeter throw backharry wait illegal\n",
            "bully maguire good\n",
            "tom holland fangirl tom holland love meme no embarrassment\n",
            "meme countless\n",
            "tobey maguire good spiderman\n",
            "peter throw grenade harry laugh hard\n",
            "favourite part movie\n",
            "-0.032240000000000005\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWdV_Eu2fm2U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dhinchak poojasholay\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkO8_E9hb7v6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
